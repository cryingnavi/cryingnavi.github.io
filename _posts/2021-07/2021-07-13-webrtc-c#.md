---
layout: post
title: Window용 WebRTC 앱 만들기
date: "2021-07-13 10:22"
categories: webrtc
tags: [webrtc, turn, C#, MixedReality, MixedReality-WebRTC, Visual Studio]
published: false
---


## 개요
샘플용 UWP 어플리케이션을 제작함으로써 MixedReality-WebRTC를 이용해 어떻게 WebRTC 서비스를 구현할 수 있는지 단계별로 알아보도록 하겠습니다. 아래 단계를 모두 거치면, UWP 데스크탑 앱과 다른 피씨 또는 모바일 브라우저와 WebRTC로 화상 통신을 연결할 수 있을 것입니다.  

또한, 시그널 서버는 자체 구현한 시그널 서버를 사용할 것입니다. 해당 시그널 서버는 https://github.com/cryingnavi/webrtc-server를 참조하시면 됩니다. 해당 서버를 구동하는 방법과 통신 방법은 별도로 설명하겠습니다.

UWP가 무엇인지에 대하 자세히 알고 싶다면, https://docs.microsoft.com/ko-kr/windows/uwp/get-started/universal-application-platform-guide?OCID=VSClient_Ver17_UWPOverview_what-is-uwp를 참조하면 됩니다.  


이 예제는 https://microsoft.github.io/MixedReality-WebRTC/manual/cs/helloworld-cs-setup-uwp.html 예제를 기반으로 하고 있습니다.


## 프로젝트 생성하기
유니버셜 프로젝트를 생성합니다. 저는 WebRTCEx02라는 이름으로 프로젝트를 생성했습니다. 
![프로젝트 생성하기](/assets/images/2021-07-12/webrtc01.png)
![프로젝트 생성하기](/assets/images/2021-07-12/webrtc02.png)

## MixedReality-WebRTC에 종속성 추가하기
MixedReality-WebRTC 종속성을 추가하기 위해서는 NuGet 패키지 도구를 이용합니다. NuGet은 Visual Studio에서 사용되는 무료 또는 오픈 소스 패키지를 관리해주는 도구로 GUI 환경에서 이를 다운로드 하고 관리할 수 있습니다.  
MixedReality-WebRTC는 NuGet을 이용해 종속성을 추가합니다. 

![종속성추가](/assets/images/2021-07-10/webrtc04.png)
![종속성추가](/assets/images/2021-07-12/webrtc03.png)

위와 같이 Microsoft.MixedReality.WebRTC를 검색해서 설치 합니다. 이번에는 반드시 Microsoft.MixedReality.WebRTC.UWP를 설치해야 합니다.

## 레이아웃 생성하기
MainPage.xaml 파일을 열고 아래와 같이 xml을 작성합니다.

```
<Page
	x:Class="WebRTCEX02.MainPage"
	xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation"
	xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml"
	xmlns:local="using:WebRTCEX02"
	xmlns:d="http://schemas.microsoft.com/expression/blend/2008"
	xmlns:mc="http://schemas.openxmlformats.org/markup-compatibility/2006"
	mc:Ignorable="d"
	Background="{ThemeResource ApplicationPageBackgroundThemeBrush}" d:DesignWidth="3833.333" d:DesignHeight="2231.481">

	<Grid>
		<Grid.ColumnDefinitions>
			<ColumnDefinition Width="5*"/>
			<ColumnDefinition Width="5*"/>
		</Grid.ColumnDefinitions>
		<Grid.RowDefinitions>
			<RowDefinition Height="5*"/>
			<RowDefinition Height="*"/>
			<RowDefinition Height="*"/>
		</Grid.RowDefinitions>

		<Border Grid.Row="0" Grid.Column="0" Background="#000"/>
		<Border Grid.Row="0" Grid.Column="1" Background="gray"/>

		<StackPanel Grid.Row="0" Grid.Column="0" Margin="20" VerticalAlignment="Center">
			<!-- 로컬 비디오 표시 -->
		</StackPanel>
		<StackPanel Grid.Row="0" Grid.Column="1" Margin="20" VerticalAlignment="Center">
			<!-- 리모트 비디오 표시 -->
		</StackPanel>
		<StackPanel Grid.Row="1" Grid.ColumnSpan="2" Orientation="Horizontal"
			HorizontalAlignment="Center" VerticalAlignment="Center">
			<TextBox Width="500" PlaceholderText="채팅방 아이디를 입력하세요"/>
		</StackPanel>

		<StackPanel Grid.Row="2" Grid.ColumnSpan="2" Orientation="Horizontal"
			HorizontalAlignment="Center" VerticalAlignment="Center">
			<Button Content="Start" Margin="10" />
			<Button Content="Call" Margin="10" />
			<Button Content="HangUp" Margin="10" />
		</StackPanel>
	</Grid>
</Page>
```

그럼 아래와 같은 레이아웃이 생성 될 것입니다. 검은색 바탕은 로컬 비디오가 표현될 공간이고 회색 바탕은 리모트 비디오가 표현될 공간입니다.

![레이아웃](/assets/images/2021-07-12/webrtc04.png)

## 권한 허용하기
WebRTC 어플리케이션은 웹캠, 마이크, 인터넷 액세스에 대한 권한을 가지고 있어야 합니다. 웹캠과 마이크는 당연히 장치에 접근해야하기 때문이고 인터넷은 다른 피어와 연결해야하기 때문에 권한 허용이 필요한 부분입니다.

![레이아웃](/assets/images/2021-07-12/webrtc05.png)
![레이아웃](/assets/images/2021-07-12/webrtc06.png)


## 로컬 비디오 표현하기
로컬 비디오를 표시하기 위해서는 VideoBridge라는 유틸리티가 필요합니다. 이는 캡처한 비디오 프레임을 수집해서 표시하는 역할을 수행합니다. 또한 VideoBridge는 StreamSamplePool 클래스를 사용합니다. 두 개의 소스는 각각 
https://github.com/microsoft/MixedReality-WebRTC/blob/master/examples/TestAppUwp/Video/VideoBridge.cs 와 https://github.com/microsoft/MixedReality-WebRTC/blob/master/examples/TestAppUwp/Video/StreamSamplePool.cs 에서 얻을 수 있습니다.

두개의 소스를 가져와 프로젝트에 포함 시킵니다.

![로컬 비디오](/assets/images/2021-07-12/webrtc07.png)

### 코딩하기
코딩할 게 많습니다. 일단 Loaded 이벤트를 추가합니다. Loaded 이벤트는 어플리케이션이 실행되고 페이지가 로드 되면 호출됩니다. 웹에선 DOM에 접근할 수 있는 onload와 같습니다.

```
public MainPage()
{
	this.InitializeComponent();

	this.Loaded += OnLoaded;
}
```

### Loaded 이벤트 작성
아래는 Loaded 이벤트의 전체 소스 코드입니다. 
```
private async void OnLoaded(object sender, RoutedEventArgs e) {
	DeviceAudioTrackSource _microphoneSource;
	DeviceVideoTrackSource _webcamSource;
	LocalAudioTrack _localAudioTrack;
	LocalVideoTrack _localVideoTrack;

	_microphoneSource = await DeviceAudioTrackSource.CreateAsync();
	_webcamSource = await DeviceVideoTrackSource.CreateAsync();

	var audioTrackConfig = new LocalAudioTrackInitConfig {
		trackName = "microphone_track"
	};
	_localAudioTrack = LocalAudioTrack.CreateFromSource(_microphoneSource, audioTrackConfig);

	var videoTrackConfig = new LocalVideoTrackInitConfig {
		trackName = "webcam_track"
	};
	_localVideoTrack = LocalVideoTrack.CreateFromSource(_webcamSource, videoTrackConfig);

	_webcamSource.I420AVideoFrameReady += LocalI420AFrameReady;
}
```

아래는 미디어 소스와 트랙을 저장하기 위한 변수 입니다.
```
DeviceAudioTrackSource _microphoneSource;
DeviceVideoTrackSource _webcamSource;
LocalAudioTrack _localAudioTrack;
LocalVideoTrack _localVideoTrack;
```

CreateAsync() 메소드로 미디어 소스를 얻어냅니다. 여기서는 인자로 아무것도 전달하지 않아 미디어 옵션이 기본값으로 사용됩니다.   
```
_microphoneSource = await DeviceAudioTrackSource.CreateAsync();
_webcamSource = await DeviceVideoTrackSource.CreateAsync();
```

만약 미디어에 대한 설정을 해야한다면 LocalAudioDeviceInitConfig, LocalVideoDeviceInitConfig를 사용할 수 있습니다. 사용법은 다음과 같습니다.

```
_microphoneSource = await DeviceAudioTrackSource.CreateAsync(new LocalAudioDeviceInitConfig() {
	AutoGainControl = true
});
_webcamSource = await DeviceVideoTrackSource.CreateAsync(new LocalVideoDeviceInitConfig() {
	framerate = 30.0,
	width = 640,
	enableMrc = true
});
```

- LocalAudioDeviceInitConfig

| 이름 | 설명 |
|------|------|
| AutoGainControl | AGC를 켜서 게인을 자동으로 조절합니다. 곧 강한 신호는 약하게 약한 신호는 강하게 증폭하게 됩니다. |


- LocalVideoDeviceInitConfig

| 이름 | 설명 |
|------|------|
| enableMrc | 홀로렌즈와 같은 혼합현실을 제공하는 장치에서 가상과 현실을 모두 캡쳐할지 여부를 지정함 |
| enableMrcRecordingIndicator | enableMrc가 true인경우, 카메라가 촬영하는 동안 적색원을 표시함 |
| framerate | 프레임레이트를 조절함 |
| width | 해상도 지정 |
| height | 해상도 지정 |
| videoDevice | 비디오 캡처 장치, 여러 비디오 장치가 있을 경우 그중 하나를 선택할 수 있다. |
| videoProfileId |  |
| videoProfileKind |  |


이제 비디오 트랙과 오디오 트랙을 생성합니다.
```
var audioTrackConfig = new LocalAudioTrackInitConfig {
	trackName = "microphone_track"
};
_localAudioTrack = LocalAudioTrack.CreateFromSource(_microphoneSource, audioTrackConfig);

var videoTrackConfig = new LocalVideoTrackInitConfig {
	trackName = "webcam_track"
};
_localVideoTrack = LocalVideoTrack.CreateFromSource(_webcamSource, videoTrackConfig);
_webcamSource.I420AVideoFrameReady += LocalI420AFrameReady;
```
생성한 미디어 트랙은 피어 커넥션 객체의 AddTransceiver메소드의 인자로 지정될 것입니다. Transceiver는 로컬과 원격 미디어를 연결하는 파이프로 피어간에 미디어를 전송하는 역할을 수행합니다.

I420AVideoFrameReady 이벤트는 비디오 소스가 생성되고 첫번째 프레임이 도착하면 호출되는 이벤트입니다. 해당 이벤트 안에서 비디오 엘리먼트와 매칭하여 비디오를 재생합니다.

### I420AVideoFrameReady 이벤트 작성
```
private void LocalI420AFrameReady(I420AVideoFrame frame) {
	lock (_localVideoLock) {
		if (!_localVideoPlaying) {
			_localVideoPlaying = true;

			// Capture the resolution into local variable useable from the lambda below
			uint width = frame.width;
			uint height = frame.height;

			// Defer UI-related work to the main UI thread
			RunOnMainThread(() => {
				// Bridge the local video track with the local media player UI
				int framerate = 30; // assumed, for lack of an actual value
				_localVideoSource = CreateI420VideoStreamSource(width, height, framerate);

				var localVideoPlayer = new MediaPlayer();
				localVideoPlayer.Source = MediaSource.CreateFromMediaStreamSource(_localVideoSource);

				localVideoPlayerElement.SetMediaPlayer(localVideoPlayer);
				localVideoPlayer.Play();
			});
		}
	}
	// Enqueue the incoming frame into the video bridge; the media player will
	// later dequeue it as soon as it's ready.
	_localVideoBridge.HandleIncomingVideoFrame(frame);
}
```

I420AVideoFrameReady 이벤트는 비디오 소스가 생성되고 첫번째 프레임이 도착하면 호출되는 이벤트입니다. 해당 이벤트 안에서 비디오 엘리먼트와 매칭하여 비디오를 재생합니다.

CreateI420VideoStreamSource 메소드는 WebRTC가 원시 비디오 프레임으로 제공하는 인코딩인 I420 형식으로 비디오 스트림을 반환하는 메소드입니다.

MediaPlayer를 생성하여 CreateI420VideoStreamSource에 의해 I420 형식으로 변환된 미디어 소스를 지정합니다.

그리고 마지막으로 xaml에서 생성한 MediaPlayerElement인 localVideoPlayerElement과 localVideoPlayer를 연결하여 재생합니다.

여기까지 레이아웃을 생성하고 기본 설정을 통해 로컬 미디어를 재생했습니다. 다음 시간에는 본격적으로 peer to peer 연결을 수행해보겠습니다.


### 시그널 서버 실행하기
먼저 시그널 서버를 실행합니다. 시그널 서버는 제가 예전에 작성해놓은 Node.js 서버를 이용하겠습니다. 시그널 서버 깃 주소는 https://github.com/cryingnavi/webrtc-server와 같습니다. 프로젝트를 내려 받아 <code>npm run start</code>로 서버를 실행하면 localhost:11200으로 실행됩니다.

해당 서버는 한개의 REST API를 제공합니다. http://localhost:11200/roomReady 인데 이를 이용해 peer가 접속할 수 있는 채팅방을 생성할 수 있습니다.  
이 API로 채팅방을 생성하고 websocket으로 해당 채팅방에 접속하겠다는 커맨드를 전송합니다, 그리고 해당 채팅방 내의 Peer간 SDP와 Candidate를 교환하면 최종 P2P가 연결됩니다.


### REST API 요청 코드 작성
우선 JSON 오브젝트 파싱을 위해 Nuget을 통해 Newtonsoft.Json를 다운 받아 설치합니다.
```
private async void roomJoin() {
	var client = new HttpClient();
	var response = await client.GetAsync("http://localhost:11200/roomReady");

	string result = response.Content.ReadAsStringAsync().Result;

	dynamic data = JObject.Parse(result);
	Console.Write(data.body.roomId);

	roomId.Text = data.body.roomId;
}

private async void call(object sender, RoutedEventArgs e) {
	this.roomJoin();
}

Call 버튼을 클릭하면 webrtc-server를 호출하여 roomId를 할당 받습니다. 그리고 이를 텍스트 박스에 표시합니다.
```

### WebSocket 클라이언트 코드 작성
webrtc-server는 Peer간의 sdp, candidate를 교환하기 위해 웹소켓을 사용합니다. 


